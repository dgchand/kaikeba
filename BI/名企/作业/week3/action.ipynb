{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action1\t\n",
    "针对Delicious数据集，对SimpleTagBased算法进行改进（使用NormTagBased、TagBased-TFIDF算法\t\n",
    "完成代码，结果正确（20points）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始数据加载...\n",
      "数据集大小为 437593.\n",
      "设置tag的人数 1867.\n",
      "数据加载完成\n",
      "\n",
      "训练集样本数 1860, 测试集样本数 1793\n",
      "user_tags, tag_items, user_items初始化完成.\n",
      "user_tags大小 1860, tag_items大小 36884, user_items大小 1860\n",
      "推荐结果评估\n",
      "  N        精确率        召回率\n",
      "  5      0.829%      0.355%\n",
      " 10      0.633%      0.542%\n",
      " 20      0.512%      0.877%\n",
      " 40      0.381%      1.304%\n",
      " 60      0.318%      1.635%\n",
      " 80      0.276%      1.893%\n",
      "100      0.248%      2.124%\n"
     ]
    }
   ],
   "source": [
    "# 使用SimpleTagBased算法对Delicious2K数据进行推荐\n",
    "# 原始数据集：https://grouplens.org/datasets/hetrec-2011/\n",
    "# 数据格式：userID     bookmarkID     tagID     timestamp\n",
    "import random\n",
    "import math\n",
    "import operator\n",
    "import pandas as pd\n",
    "\n",
    "file_path = \"./user_taggedbookmarks-timestamps.dat\"\n",
    "# 字典类型，保存了user对item的tag，即{userid: {item1:[tag1, tag2], ...}}\n",
    "records = {}\n",
    "# 训练集，测试集\n",
    "train_data = dict()\n",
    "test_data = dict()\n",
    "# 用户标签，商品标签\n",
    "user_tags = dict()\n",
    "tag_items = dict()\n",
    "user_items = dict()\n",
    "\n",
    "# 使用测试集，计算准确率和召回率\n",
    "def precisionAndRecall(N):\n",
    "    hit = 0\n",
    "    h_recall = 0\n",
    "    h_precision = 0\n",
    "    for user,items in test_data.items():\n",
    "        if user not in train_data:\n",
    "            continue\n",
    "        # 获取Top-N推荐列表\n",
    "        rank = recommend(user, N)\n",
    "        for item,rui in rank:\n",
    "            if item in items:\n",
    "                hit = hit + 1\n",
    "        h_recall = h_recall + len(items)\n",
    "        h_precision = h_precision + N\n",
    "    #print('一共命中 %d 个, 一共推荐 %d 个, 用户设置tag总数 %d 个' %(hit, h_precision, h_recall))\n",
    "    # 返回准确率 和 召回率\n",
    "    return (hit/(h_precision*1.0)), (hit/(h_recall*1.0))\n",
    "\n",
    "# 对用户user推荐Top-N\n",
    "def recommend(user, N):\n",
    "    recommend_items=dict()\n",
    "    # 对Item进行打分，分数为所有的（用户对某标签使用的次数 wut, 乘以 商品被打上相同标签的次数 wti）之和\n",
    "    tagged_items = user_items[user]     \n",
    "    for tag, wut in user_tags[user].items():\n",
    "        #print(self.user_tags[user].items())\n",
    "        for item, wti in tag_items[tag].items():\n",
    "            if item in tagged_items:\n",
    "                continue\n",
    "            #print('wut = %s, wti = %s' %(wut, wti))\n",
    "            if item not in recommend_items:\n",
    "                recommend_items[item] = wut * wti\n",
    "            else:\n",
    "                recommend_items[item] = recommend_items[item] + wut * wti\n",
    "    return sorted(recommend_items.items(), key=operator.itemgetter(1), reverse=True)[0:N]\n",
    "\n",
    "# 使用测试集，对推荐结果进行评估\n",
    "def testRecommend():\n",
    "    print(\"推荐结果评估\")\n",
    "    print(\"%3s %10s %10s\" % ('N',\"精确率\",'召回率'))\n",
    "    for n in [5,10,20,40,60,80,100]:\n",
    "        precision,recall = precisionAndRecall(n)\n",
    "        print(\"%3d %10.3f%% %10.3f%%\" % (n, precision * 100, recall * 100))\n",
    "        \n",
    "# 数据加载\n",
    "def load_data():\n",
    "    print(\"开始数据加载...\")\n",
    "    df = pd.read_csv(file_path, sep='\\t')\n",
    "    for i in range(len(df)):\n",
    "        uid = df['userID'][i]\n",
    "        iid = df['bookmarkID'][i]\n",
    "        tag = df['tagID'][i]\n",
    "        # 键不存在时，设置默认值{}\n",
    "        records.setdefault(uid,{})\n",
    "        records[uid].setdefault(iid,[])\n",
    "        records[uid][iid].append(tag)\n",
    "    print(\"数据集大小为 %d.\" % (len(df)))\n",
    "    print(\"设置tag的人数 %d.\" % (len(records)))\n",
    "    print(\"数据加载完成\\n\")\n",
    "\n",
    "# 将数据集拆分为训练集和测试集\n",
    "def train_test_split(ratio, seed=100):\n",
    "    random.seed(seed)\n",
    "    for u in records.keys():\n",
    "        for i in records[u].keys():\n",
    "            # ratio比例设置为测试集\n",
    "            if random.random()<ratio:\n",
    "                test_data.setdefault(u,{})\n",
    "                test_data[u].setdefault(i,[])\n",
    "                for t in records[u][i]:\n",
    "                    test_data[u][i].append(t)\n",
    "            else:\n",
    "                train_data.setdefault(u,{})\n",
    "                train_data[u].setdefault(i,[])\n",
    "                for t in records[u][i]:\n",
    "                    train_data[u][i].append(t)        \n",
    "    print(\"训练集样本数 %d, 测试集样本数 %d\" % (len(train_data),len(test_data)))\n",
    "\n",
    "# 设置矩阵 mat[index, item] = 1\n",
    "def addValueToMat(mat, index, item, value=1):\n",
    "    if index not in mat:\n",
    "        mat.setdefault(index,{})\n",
    "        mat[index].setdefault(item,value)\n",
    "    else:\n",
    "        if item not in mat[index]:\n",
    "            mat[index][item] = value\n",
    "        else:\n",
    "            mat[index][item] += value\n",
    "\n",
    "\n",
    "# 使用训练集，初始化user_tags, tag_items, user_items\n",
    "def initStat():\n",
    "    records=train_data\n",
    "    for u,items in records.items():\n",
    "        for i,tags in items.items():\n",
    "            for tag in tags:\n",
    "                #print tag\n",
    "                # 用户和tag的关系\n",
    "                addValueToMat(user_tags, u, tag, 1)\n",
    "                # tag和item的关系\n",
    "                addValueToMat(tag_items, tag, i, 1)\n",
    "                # 用户和item的关系\n",
    "                addValueToMat(user_items, u, i, 1)\n",
    "    print(\"user_tags, tag_items, user_items初始化完成.\")\n",
    "    print(\"user_tags大小 %d, tag_items大小 %d, user_items大小 %d\" % (len(user_tags), len(tag_items), len(user_items)))\n",
    "\n",
    "# 数据加载\n",
    "load_data()\n",
    "# 训练集，测试集拆分，20%测试集\n",
    "train_test_split(0.2)\n",
    "initStat()\n",
    "testRecommend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始数据加载...\n",
      "数据集大小为 437593.\n",
      "设置tag的人数 1867.\n",
      "数据加载完成\n",
      "\n",
      "训练集样本数 1860, 测试集样本数 1793\n",
      "user_tags, tag_items, user_items，item_tags, tag_users, item_users初始化完成.\n",
      "user_tags大小 1860, tag_items大小 36884, user_items大小 1860, item_tags大小 59555，, tag_users大小 36884, item_users大小 59555 \n",
      "推荐结果评估\n",
      "  N        精确率        召回率\n",
      "  5      0.907%      0.388%\n",
      " 10      0.638%      0.546%\n",
      " 20      0.507%      0.868%\n",
      " 40      0.356%      1.218%\n",
      " 60      0.287%      1.476%\n",
      " 80      0.255%      1.750%\n",
      "100      0.241%      2.061%\n"
     ]
    }
   ],
   "source": [
    "# 使用NormTagBased算法对Delicious2K数据进行推荐\n",
    "# 原始数据集：https://grouplens.org/datasets/hetrec-2011/\n",
    "# 数据格式：userID     bookmarkID     tagID     timestamp\n",
    "import random\n",
    "import math\n",
    "import operator\n",
    "import pandas as pd\n",
    "\n",
    "file_path = \"./user_taggedbookmarks-timestamps.dat\"\n",
    "# 字典类型，保存了user对item的tag，即{userid: {item1:[tag1, tag2], ...}}\n",
    "records = {}\n",
    "# 训练集，测试集\n",
    "train_data = dict()\n",
    "test_data = dict()\n",
    "# 用户标签，商品标签\n",
    "user_tags = dict()\n",
    "tag_items = dict()\n",
    "user_items = dict()\n",
    "item_tags = dict()\n",
    "tag_users = dict()\n",
    "item_users = dict()\n",
    "\n",
    "# 使用测试集，计算准确率和召回率\n",
    "def precisionAndRecall(N):\n",
    "    hit = 0\n",
    "    h_recall = 0\n",
    "    h_precision = 0\n",
    "    for user,items in test_data.items():\n",
    "        if user not in train_data:\n",
    "            continue\n",
    "        # 获取Top-N推荐列表\n",
    "        rank = recommend(user, N)\n",
    "        for item,rui in rank:\n",
    "            if item in items:\n",
    "                hit = hit + 1\n",
    "        h_recall = h_recall + len(items)\n",
    "        h_precision = h_precision + N\n",
    "    #print('一共命中 %d 个, 一共推荐 %d 个, 用户设置tag总数 %d 个' %(hit, h_precision, h_recall))\n",
    "    # 返回准确率 和 召回率\n",
    "    return (hit/(h_precision*1.0)), (hit/(h_recall*1.0))\n",
    "\n",
    "# 对用户user推荐Top-N\n",
    "def recommend(user, N):\n",
    "    recommend_items=dict()\n",
    "    # 对Item进行打分，分数为所有的（用户对某标签使用的次数 wut, 乘以 商品被打上相同标签的次数 wti）之和\n",
    "    tagged_items = user_items[user]     \n",
    "    for tag, wut in user_tags[user].items():\n",
    "        #print(self.user_tags[user].items())\n",
    "        for item, wti in tag_items[tag].items():\n",
    "            if item in tagged_items:\n",
    "                continue\n",
    "            ## NormTagBased\n",
    "            norm = len(tag_users[tag].items()) * len(user_tags[user].items())\n",
    "            #print('wut = %s, wti = %s' %(wut, wti))\n",
    "            if item not in recommend_items:\n",
    "                recommend_items[item] = wut * wti / norm\n",
    "            else:\n",
    "                recommend_items[item] = recommend_items[item] + wut * wti/norm\n",
    "    return sorted(recommend_items.items(), key=operator.itemgetter(1), reverse=True)[0:N]\n",
    "\n",
    "# 使用训练集，初始化user_tags, tag_items, user_items, item_tags, tag_users, item_users\n",
    "def initStat():\n",
    "    records=train_data\n",
    "    for u,items in records.items():\n",
    "        for i,tags in items.items():\n",
    "            for tag in tags:\n",
    "                #print tag\n",
    "                # 用户和tag的关系\n",
    "                addValueToMat(user_tags, u, tag, 1)\n",
    "                # tag和item的关系\n",
    "                addValueToMat(tag_items, tag, i, 1)\n",
    "                # 用户和item的关系\n",
    "                addValueToMat(user_items, u, i, 1)\n",
    "                # item和tag\n",
    "                addValueToMat(item_tags, i, tag, 1)\n",
    "                # tag和user\n",
    "                addValueToMat(tag_users, tag, u, 1)\n",
    "                # item和user\n",
    "                addValueToMat(item_users, i, u, 1)\n",
    "    print(\"user_tags, tag_items, user_items，item_tags, tag_users, item_users初始化完成.\")\n",
    "    print(\"user_tags大小 %d, tag_items大小 %d, user_items大小 %d, item_tags大小 %d，, tag_users大小 %d, item_users大小 %d \" \\\n",
    "          % (len(user_tags), len(tag_items), len(user_items), len(item_tags), len(tag_users), len(item_users)))\n",
    "\n",
    "# 数据加载\n",
    "load_data()\n",
    "# 训练集，测试集拆分，20%测试集\n",
    "train_test_split(0.2)\n",
    "initStat()\n",
    "testRecommend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始数据加载...\n",
      "数据集大小为 437593.\n",
      "设置tag的人数 1867.\n",
      "数据加载完成\n",
      "\n",
      "训练集样本数 1860, 测试集样本数 1793\n",
      "user_tags, tag_items, user_items，item_tags, tag_users, item_users初始化完成.\n",
      "user_tags大小 1860, tag_items大小 36884, user_items大小 1860, item_tags大小 59555，, tag_users大小 36884, item_users大小 59555 \n",
      "推荐结果评估\n",
      "  N        精确率        召回率\n",
      "  5      1.008%      0.431%\n",
      " 10      0.761%      0.652%\n",
      " 20      0.549%      0.940%\n",
      " 40      0.402%      1.376%\n",
      " 60      0.328%      1.687%\n",
      " 80      0.297%      2.033%\n",
      "100      0.269%      2.306%\n"
     ]
    }
   ],
   "source": [
    "# 使用TagBased-IDF算法对Delicious2K数据进行推荐\n",
    "# 原始数据集：https://grouplens.org/datasets/hetrec-2011/\n",
    "# 数据格式：userID     bookmarkID     tagID     timestamp\n",
    "import random\n",
    "import math\n",
    "import operator\n",
    "import pandas as pd\n",
    "\n",
    "file_path = \"./user_taggedbookmarks-timestamps.dat\"\n",
    "# 字典类型，保存了user对item的tag，即{userid: {item1:[tag1, tag2], ...}}\n",
    "records = {}\n",
    "# 训练集，测试集\n",
    "train_data = dict()\n",
    "test_data = dict()\n",
    "# 用户标签，商品标签\n",
    "user_tags = dict()\n",
    "tag_items = dict()\n",
    "user_items = dict()\n",
    "item_tags = dict()\n",
    "tag_users = dict()\n",
    "item_users = dict()\n",
    "\n",
    "# 对用户user推荐Top-N\n",
    "def recommend(user, N):\n",
    "    recommend_items=dict()\n",
    "    # 对Item进行打分，分数为所有的（用户对某标签使用的次数 wut, 乘以 商品被打上相同标签的次数 wti）之和\n",
    "    tagged_items = user_items[user]     \n",
    "    for tag, wut in user_tags[user].items():\n",
    "        #print(self.user_tags[user].items())\n",
    "        for item, wti in tag_items[tag].items():\n",
    "            if item in tagged_items:\n",
    "                continue\n",
    "            ## TagBased-IDF\n",
    "            norm = math.log(len(tag_users[tag].items())+1)\n",
    "            #print('wut = %s, wti = %s' %(wut, wti))\n",
    "            if item not in recommend_items:\n",
    "                recommend_items[item] = wut * wti / norm\n",
    "            else:\n",
    "                recommend_items[item] = recommend_items[item] + wut * wti/norm\n",
    "    return sorted(recommend_items.items(), key=operator.itemgetter(1), reverse=True)[0:N]\n",
    "\n",
    "# 使用训练集，初始化user_tags, tag_items, user_items, item_tags, tag_users, item_users\n",
    "def initStat():\n",
    "    records=train_data\n",
    "    for u,items in records.items():\n",
    "        for i,tags in items.items():\n",
    "            for tag in tags:\n",
    "                #print tag\n",
    "                # 用户和tag的关系\n",
    "                addValueToMat(user_tags, u, tag, 1)\n",
    "                # tag和item的关系\n",
    "                addValueToMat(tag_items, tag, i, 1)\n",
    "                # 用户和item的关系\n",
    "                addValueToMat(user_items, u, i, 1)\n",
    "                # item和tag\n",
    "                addValueToMat(item_tags, i, tag, 1)\n",
    "                # tag和user\n",
    "                addValueToMat(tag_users, tag, u, 1)\n",
    "                # item和user\n",
    "                addValueToMat(item_users, i, u, 1)\n",
    "    print(\"user_tags, tag_items, user_items，item_tags, tag_users, item_users初始化完成.\")\n",
    "    print(\"user_tags大小 %d, tag_items大小 %d, user_items大小 %d, item_tags大小 %d，, tag_users大小 %d, item_users大小 %d \" \\\n",
    "          % (len(user_tags), len(tag_items), len(user_items), len(item_tags), len(tag_users), len(item_users)))\n",
    "\n",
    "\n",
    "# 数据加载\n",
    "load_data()\n",
    "# 训练集，测试集拆分，20%测试集\n",
    "train_test_split(0.2)\n",
    "initStat()\n",
    "testRecommend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action2\t\n",
    "\"对Titanic数据进行清洗，建模并对乘客生存进行预测。使用之前介绍过的10种模型中的至少2种（包括TPOT）\n",
    "\n",
    "1、完成代码，结果正确（20points）\n",
    "2、能使用TPOT完成预测（20points）\"!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查看数据信息：列名、非空个数、类型等\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            714 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       889 non-null object\n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n",
      "None\n",
      "------------------------------\n",
      "查看数据摘要\n",
      "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
      "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
      "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
      "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
      "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
      "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
      "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
      "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
      "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
      "\n",
      "            Parch        Fare  \n",
      "count  891.000000  891.000000  \n",
      "mean     0.381594   32.204208  \n",
      "std      0.806057   49.693429  \n",
      "min      0.000000    0.000000  \n",
      "25%      0.000000    7.910400  \n",
      "50%      0.000000   14.454200  \n",
      "75%      0.000000   31.000000  \n",
      "max      6.000000  512.329200  \n",
      "------------------------------\n",
      "查看离散数据分布\n",
      "                                Name   Sex Ticket    Cabin Embarked\n",
      "count                            891   891    891      204      889\n",
      "unique                           891     2    681      147        3\n",
      "top     Carlsson, Mr. August Sigfrid  male   1601  B96 B98        S\n",
      "freq                               1   577      7        4      644\n",
      "------------------------------\n",
      "查看前5条数据\n",
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n",
      "------------------------------\n",
      "查看后5条数据\n",
      "     PassengerId  Survived  Pclass                                      Name  \\\n",
      "886          887         0       2                     Montvila, Rev. Juozas   \n",
      "887          888         1       1              Graham, Miss. Margaret Edith   \n",
      "888          889         0       3  Johnston, Miss. Catherine Helen \"Carrie\"   \n",
      "889          890         1       1                     Behr, Mr. Karl Howell   \n",
      "890          891         0       3                       Dooley, Mr. Patrick   \n",
      "\n",
      "        Sex   Age  SibSp  Parch      Ticket   Fare Cabin Embarked  \n",
      "886    male  27.0      0      0      211536  13.00   NaN        S  \n",
      "887  female  19.0      0      0      112053  30.00   B42        S  \n",
      "888  female   NaN      1      2  W./C. 6607  23.45   NaN        S  \n",
      "889    male  26.0      0      0      111369  30.00  C148        C  \n",
      "890    male  32.0      0      0      370376   7.75   NaN        Q  \n",
      "S    644\n",
      "C    168\n",
      "Q     77\n",
      "Name: Embarked, dtype: int64\n",
      "特征值\n",
      "     Pclass     Sex        Age  SibSp  Parch     Fare Embarked\n",
      "0         3    male  22.000000      1      0   7.2500        S\n",
      "1         1  female  38.000000      1      0  71.2833        C\n",
      "2         3  female  26.000000      0      0   7.9250        S\n",
      "3         1  female  35.000000      1      0  53.1000        S\n",
      "4         3    male  35.000000      0      0   8.0500        S\n",
      "..      ...     ...        ...    ...    ...      ...      ...\n",
      "886       2    male  27.000000      0      0  13.0000        S\n",
      "887       1  female  19.000000      0      0  30.0000        S\n",
      "888       3  female  29.699118      1      2  23.4500        S\n",
      "889       1    male  26.000000      0      0  30.0000        C\n",
      "890       3    male  32.000000      0      0   7.7500        Q\n",
      "\n",
      "[891 rows x 7 columns]\n",
      "['Age', 'Embarked=C', 'Embarked=Q', 'Embarked=S', 'Fare', 'Parch', 'Pclass', 'Sex=female', 'Sex=male', 'SibSp']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# 数据加载\n",
    "train_data = pd.read_csv('./train.csv')\n",
    "test_data = pd.read_csv('./test.csv')\n",
    "# 数据探索\n",
    "# 查看train_data信息\n",
    "#pd.set_option('display.max_columns', None) #显示所有列\n",
    "print('查看数据信息：列名、非空个数、类型等')\n",
    "print(train_data.info())\n",
    "print('-'*30)\n",
    "print('查看数据摘要')\n",
    "print(train_data.describe())\n",
    "print('-'*30)\n",
    "print('查看离散数据分布')\n",
    "print(train_data.describe(include=['O']))\n",
    "print('-'*30)\n",
    "print('查看前5条数据')\n",
    "print(train_data.head())\n",
    "print('-'*30)\n",
    "print('查看后5条数据')\n",
    "print(train_data.tail())\n",
    "\n",
    "# 使用平均年龄来填充年龄中的nan值\n",
    "train_data['Age'].fillna(train_data['Age'].mean(), inplace=True)\n",
    "test_data['Age'].fillna(test_data['Age'].mean(),inplace=True)\n",
    "# 使用票价的均值填充票价中的nan值\n",
    "train_data['Fare'].fillna(train_data['Fare'].mean(), inplace=True)\n",
    "test_data['Fare'].fillna(test_data['Fare'].mean(),inplace=True)\n",
    "\n",
    "print(train_data['Embarked'].value_counts())\n",
    "# 使用登录最多的港口来填充登录港口的nan值\n",
    "train_data['Embarked'].fillna('S', inplace=True)\n",
    "test_data['Embarked'].fillna('S',inplace=True)\n",
    "\n",
    "\n",
    "# 特征选择\n",
    "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
    "train_features = train_data[features]\n",
    "train_labels = train_data['Survived']\n",
    "test_features = test_data[features]\n",
    "print('特征值')\n",
    "print(train_features)\n",
    "\n",
    "dvec=DictVectorizer(sparse=False)\n",
    "train_features=dvec.fit_transform(train_features.to_dict(orient='record'))\n",
    "print(dvec.feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score准确率为 0.9820\n",
      "cross_val_score准确率为 0.7768\n"
     ]
    }
   ],
   "source": [
    "# 构造ID3决策树\n",
    "clf = DecisionTreeClassifier(criterion='entropy')\n",
    "# 决策树训练\n",
    "clf.fit(train_features, train_labels)\n",
    "\n",
    "test_features=dvec.transform(test_features.to_dict(orient='record'))\n",
    "# 决策树预测\n",
    "pred_labels = clf.predict(test_features)\n",
    "\n",
    "# 得到决策树准确率(基于训练集)\n",
    "acc_decision_tree = round(clf.score(train_features, train_labels), 6)\n",
    "print(u'score准确率为 %.4lf' % acc_decision_tree)\n",
    "\n",
    "# 使用K折交叉验证 统计决策树准确率\n",
    "print(u'cross_val_score准确率为 %.4lf' % np.mean(cross_val_score(clf, train_features, train_labels, cv=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Leigh/anaconda3/lib/python3.6/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.\n",
      "  warnings.warn(\"Warning: optional dependency `torch` is not available. - skipping import of NN models.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: xgboost.XGBClassifier is not available and will not be used by TPOT.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Optimization Progress', max=120.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: 0.8193208210407382\n",
      "Generation 2 - Current best internal CV score: 0.8237963718536188\n",
      "Generation 3 - Current best internal CV score: 0.8339087314041805\n",
      "Generation 4 - Current best internal CV score: 0.8339087314041805\n",
      "Generation 5 - Current best internal CV score: 0.8339150084740442\n",
      "Best pipeline: RandomForestClassifier(ExtraTreesClassifier(input_matrix, bootstrap=True, criterion=entropy, max_features=0.6000000000000001, min_samples_leaf=13, min_samples_split=9, n_estimators=100), bootstrap=False, criterion=entropy, max_features=0.6500000000000001, min_samples_leaf=15, min_samples_split=16, n_estimators=100)\n",
      "0.8574635241301908\n"
     ]
    }
   ],
   "source": [
    "# 使用TPOT自动机器学习工具对MNIST进行分类\n",
    "from tpot import TPOTClassifier\n",
    "\n",
    "tpot = TPOTClassifier(generations=5, population_size=20, verbosity=2)\n",
    "tpot.fit(train_features, train_labels)\n",
    "print(tpot.score(train_features, train_labels))\n",
    "tpot.export('tpot_mnist_pipeline.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tpot\n",
      "  Downloading TPOT-0.11.5-py3-none-any.whl (82 kB)\n",
      "\u001b[K     |████████████████████████████████| 82 kB 137 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.16.3 in /Users/Leigh/anaconda3/lib/python3.6/site-packages (from tpot) (1.17.2)\n",
      "Requirement already satisfied: joblib>=0.13.2 in /Users/Leigh/anaconda3/lib/python3.6/site-packages (from tpot) (0.14.1)\n",
      "Collecting deap>=1.2\n",
      "  Downloading deap-1.3.1-cp36-cp36m-macosx_10_13_x86_64.whl (109 kB)\n",
      "\u001b[K     |████████████████████████████████| 109 kB 460 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.3.1 in /Users/Leigh/anaconda3/lib/python3.6/site-packages (from tpot) (1.4.1)\n",
      "Requirement already satisfied: pandas>=0.24.2 in /Users/Leigh/anaconda3/lib/python3.6/site-packages (from tpot) (0.25.2)\n",
      "Collecting tqdm>=4.36.1\n",
      "  Downloading tqdm-4.48.2-py2.py3-none-any.whl (68 kB)\n",
      "\u001b[K     |████████████████████████████████| 68 kB 156 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=0.22.0 in /Users/Leigh/anaconda3/lib/python3.6/site-packages (from tpot) (0.22.2.post1)\n",
      "Collecting stopit>=1.1.1\n",
      "  Downloading stopit-1.1.2.tar.gz (18 kB)\n",
      "Collecting update-checker>=0.16\n",
      "  Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/Leigh/anaconda3/lib/python3.6/site-packages (from pandas>=0.24.2->tpot) (2019.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /Users/Leigh/anaconda3/lib/python3.6/site-packages (from pandas>=0.24.2->tpot) (2.8.0)\n",
      "Requirement already satisfied: requests>=2.3.0 in /Users/Leigh/anaconda3/lib/python3.6/site-packages (from update-checker>=0.16->tpot) (2.23.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/Leigh/anaconda3/lib/python3.6/site-packages (from python-dateutil>=2.6.1->pandas>=0.24.2->tpot) (1.12.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/Leigh/anaconda3/lib/python3.6/site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/Leigh/anaconda3/lib/python3.6/site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/Leigh/anaconda3/lib/python3.6/site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/Leigh/anaconda3/lib/python3.6/site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (1.24.2)\n",
      "Building wheels for collected packages: stopit\n",
      "  Building wheel for stopit (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for stopit: filename=stopit-1.1.2-py3-none-any.whl size=11955 sha256=1a9149ffb1758f0f4e35f9c33e926378f4cb4f8a401a5cb28369a3f72d804057\n",
      "  Stored in directory: /Users/Leigh/Library/Caches/pip/wheels/07/2e/ce/e558b7d4f9aafcdc0e5638ef890a3d5166d8a0f2c2dc768379\n",
      "Successfully built stopit\n",
      "\u001b[31mERROR: rasa 1.9.2 has requirement tqdm<4.32.0,>=4.31.0, but you'll have tqdm 4.48.2 which is incompatible.\u001b[0m\n",
      "Installing collected packages: deap, tqdm, stopit, update-checker, tpot\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.31.1\n",
      "    Uninstalling tqdm-4.31.1:\n",
      "      Successfully uninstalled tqdm-4.31.1\n",
      "Successfully installed deap-1.3.1 stopit-1.1.2 tpot-0.11.5 tqdm-4.48.2 update-checker-0.18.0\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.3 is available.\n",
      "You should consider upgrading via the '/Users/Leigh/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip install tpot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
